{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOURCE  of This Notebook\n",
    "https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/ \n",
    "#### Data file download - sign in required with registration\n",
    "https://www.kaggle.com/datasets/ysf12ff/app-store-dataset \n",
    "Once logged in, 'Download' link will be available around top-right corner\n",
    "#### In the downloaded zip file, you will upload 'train' data file to Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of dataset\n",
    "The data is about the applications available on the App Store, with their current version, the rating they received, price and genre of application and number of supported devices of each application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns\n",
    "id   Application Unique ID  \n",
    "track_name  Application Name  \n",
    "size_bytes application size in 'kb' \n",
    "currency   price currency  \n",
    "price      official price  \n",
    "rating_count_tot   total rating count  \n",
    "rating_count_ver   current version rating \n",
    "user_rating rating by user  \n",
    "user_rating_ver rating by user on current version  \n",
    "ver current version on app store  \n",
    "cont_rating  \n",
    "prime_genre  \n",
    "sup_devices.num  \n",
    "ipadSc_urls.num  \n",
    "lang.num  \n",
    "vpp_lic  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Apache Spark Context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the DataFrame from CSV file\n",
    "#### This is where you import your own csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sqlContext.read.csv(\"AppleStore.csv\", header = True, inferSchema = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to see datatype of columns?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- size_bytes: long (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- rating_count_tot: integer (nullable = true)\n",
      " |-- rating_count_ver: integer (nullable = true)\n",
      " |-- user_rating: double (nullable = true)\n",
      " |-- user_rating_ver: double (nullable = true)\n",
      " |-- ver: string (nullable = true)\n",
      " |-- cont_rating: string (nullable = true)\n",
      " |-- prime_genre: string (nullable = true)\n",
      " |-- sup_devices.num: integer (nullable = true)\n",
      " |-- ipadSc_urls.num: integer (nullable = true)\n",
      " |-- lang.num: integer (nullable = true)\n",
      " |-- vpp_lic: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Show first n observation? \n",
    "We can use head operation to see first n observation (say, 5 observation). Head operation in PySpark is similar to head operation in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=284882215, track_name='Facebook', size_bytes=389879808, currency='USD', price=0.0, rating_count_tot=2974676, rating_count_ver=212, user_rating=3.5, user_rating_ver=3.5, ver='95', cont_rating='4+', prime_genre='Social Networking', sup_devices.num=37, ipadSc_urls.num=1, lang.num=29, vpp_lic=1),\n",
       " Row(id=389801252, track_name='Instagram', size_bytes=113954816, currency='USD', price=0.0, rating_count_tot=2161558, rating_count_ver=1289, user_rating=4.5, user_rating_ver=4.0, ver='10.23', cont_rating='12+', prime_genre='Photo & Video', sup_devices.num=37, ipadSc_urls.num=0, lang.num=29, vpp_lic=1),\n",
       " Row(id=529479190, track_name='Clash of Clans', size_bytes=116476928, currency='USD', price=0.0, rating_count_tot=2130805, rating_count_ver=579, user_rating=4.5, user_rating_ver=4.5, ver='9.24.12', cont_rating='9+', prime_genre='Games', sup_devices.num=38, ipadSc_urls.num=5, lang.num=18, vpp_lic=1),\n",
       " Row(id=420009108, track_name='Temple Run', size_bytes=65921024, currency='USD', price=0.0, rating_count_tot=1724546, rating_count_ver=3842, user_rating=4.5, user_rating_ver=4.0, ver='1.6.2', cont_rating='9+', prime_genre='Games', sup_devices.num=40, ipadSc_urls.num=5, lang.num=1, vpp_lic=1),\n",
       " Row(id=284035177, track_name='Pandora - Music & Radio', size_bytes=130242560, currency='USD', price=0.0, rating_count_tot=1126879, rating_count_ver=3594, user_rating=4.0, user_rating_ver=4.5, ver='8.4.1', cont_rating='12+', prime_genre='Music', sup_devices.num=37, ipadSc_urls.num=4, lang.num=1, vpp_lic=1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above results are comprised of row like format. To see the result in more interactive manner (rows under the columns), we can use the show operation. Let’s apply show operation on train and take first 2 rows of it. We can pass the argument truncate = True to truncate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+--------+-----+----------------+----------------+-----------+---------------+-----+-----------+-----------------+---------------+---------------+--------+-------+\n",
      "|       id|track_name|size_bytes|currency|price|rating_count_tot|rating_count_ver|user_rating|user_rating_ver|  ver|cont_rating|      prime_genre|sup_devices.num|ipadSc_urls.num|lang.num|vpp_lic|\n",
      "+---------+----------+----------+--------+-----+----------------+----------------+-----------+---------------+-----+-----------+-----------------+---------------+---------------+--------+-------+\n",
      "|284882215|  Facebook| 389879808|     USD|  0.0|         2974676|             212|        3.5|            3.5|   95|         4+|Social Networking|             37|              1|      29|      1|\n",
      "|389801252| Instagram| 113954816|     USD|  0.0|         2161558|            1289|        4.5|            4.0|10.23|        12+|    Photo & Video|             37|              0|      29|      1|\n",
      "+---------+----------+----------+--------+-----+----------------+----------------+-----------+---------------+-----+-----------+-----------------+---------------+---------------+--------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(2,truncate= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Count the number of rows in DataFrame?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7197"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many columns do we have in train and test files along with their names?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,\n",
       " ['id',\n",
       "  'track_name',\n",
       "  'size_bytes',\n",
       "  'currency',\n",
       "  'price',\n",
       "  'rating_count_tot',\n",
       "  'rating_count_ver',\n",
       "  'user_rating',\n",
       "  'user_rating_ver',\n",
       "  'ver',\n",
       "  'cont_rating',\n",
       "  'prime_genre',\n",
       "  'sup_devices.num',\n",
       "  'ipadSc_urls.num',\n",
       "  'lang.num',\n",
       "  'vpp_lic'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.columns), train.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to get the summary statistics (mean, standard deviance, min ,max, count) of numerical columns in a DataFrame? \n",
    "describe operation is use to calculate the summary statistics of numerical column(s) in DataFrame. If we don’t specify the name of columns it will calculate summary statistics for all numerical columns present in DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------+------------------+------------------+-----------------+------------------+------------------+------------------+-----------+-----------+-----------------+------------------+-----------------+-------------------+\n",
      "|summary|                  id|          track_name|          size_bytes|currency|             price|  rating_count_tot| rating_count_ver|       user_rating|   user_rating_ver|               ver|cont_rating|prime_genre|  sup_devices.num|   ipadSc_urls.num|         lang.num|            vpp_lic|\n",
      "+-------+--------------------+--------------------+--------------------+--------+------------------+------------------+-----------------+------------------+------------------+------------------+-----------+-----------+-----------------+------------------+-----------------+-------------------+\n",
      "|  count|                7197|                7197|                7197|    7197|              7197|              7197|             7197|              7197|              7197|              7197|       7197|       7197|             7197|              7197|             7197|               7197|\n",
      "|   mean| 8.631309974515771E8|              1824.0|  1.99134453825066E8|    null|1.7262178685562666|12892.907183548701|460.3739057940809| 3.526955675976101| 3.253577879672086| 7.968864512292051|       null|       null|37.36181742392664|3.7071001806308184| 5.43490343198555| 0.9930526608309017|\n",
      "| stddev|2.7123675589291835E8|   316.7838379715733|3.5920691353870344E8|    null| 5.833005786951914| 75739.40867472615|3920.455183361982|1.5179475936298863|1.8093628231177743|107.74333834045675|       null|       null|3.737715238858465|1.9860046449596345|7.919592722881356|0.08306643356297927|\n",
      "|    min|           281656475|! OH Fantastic Fr...|              589824|     USD|               0.0|                 0|                0|               0.0|               0.0|            0.0.15|        12+|       Book|                9|                 0|                0|                  0|\n",
      "|    max|          1188375727|                 ｗｗｗ|          4025969664|     USD|            299.99|           2974676|           177050|               5.0|               5.0|            v3.6.9|         9+|    Weather|               47|                 5|               75|                  1|\n",
      "+-------+--------------------+--------------------+--------------------+--------+------------------+------------------+-----------------+------------------+------------------+------------------+-----------+-----------+-----------------+------------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|       user_rating|\n",
      "+-------+------------------+\n",
      "|  count|              7197|\n",
      "|   mean| 3.526955675976101|\n",
      "| stddev|1.5179475936298863|\n",
      "|    min|               0.0|\n",
      "|    max|               5.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.describe('user_rating').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to select column(s) from the DataFrame?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|       id|          track_name|\n",
      "+---------+--------------------+\n",
      "|284882215|            Facebook|\n",
      "|389801252|           Instagram|\n",
      "|529479190|      Clash of Clans|\n",
      "|420009108|          Temple Run|\n",
      "|284035177|Pandora - Music &...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select('id','track_name').show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if I want to calculate pair wise frequency of categorical columns? \n",
    "We can use crosstab operation on DataFrame to calculate the pair wise frequency of columns. Let’s apply crosstab operation on ‘Age’ and ‘Gender’ columns of train DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---+---+---+---+---+---+---+---+----+---+\n",
      "|prime_genre_user_rating|0.0|1.0|1.5|2.0|2.5|3.0|3.5|4.0| 4.5|5.0|\n",
      "+-----------------------+---+---+---+---+---+---+---+---+----+---+\n",
      "|               Shopping| 16|  0|  0|  0|  5| 10| 14| 24|  41| 12|\n",
      "|                 Sports| 13|  4|  1| 10| 12| 14| 22| 17|  15|  6|\n",
      "|              Lifestyle| 31|  4|  7|  4| 12| 10| 10| 29|  29|  8|\n",
      "|          Photo & Video| 24|  4|  4|  3| 16| 14| 29| 71| 154| 30|\n",
      "|                Medical|  3|  2|  1|  0|  0|  0|  3|  1|  11|  2|\n",
      "|           Productivity|  6|  0|  2|  3|  2|  5| 16| 53|  78| 13|\n",
      "|                   Book| 47|  1|  0|  0|  3|  2|  4| 11|  30| 14|\n",
      "|          Entertainment| 64|  3|  6| 21| 40| 72| 68|117| 118| 26|\n",
      "|              Reference| 11|  0|  1|  0|  1|  2|  7| 13|  21|  8|\n",
      "|                  Music|  4|  0|  0|  1|  3|  7| 17| 42|  58|  6|\n",
      "|               Catalogs|  5|  0|  0|  0|  0|  0|  1|  2|   1|  1|\n",
      "|                Weather|  6|  0|  1|  0|  2|  8| 11| 18|  24|  2|\n",
      "|              Utilities| 29|  4|  6| 10| 13| 23| 33| 56|  62| 12|\n",
      "|               Business|  4|  0|  0|  1|  5|  2|  4| 15|  22|  4|\n",
      "|                  Games|462| 15| 10| 29| 44|133|301|927|1664|277|\n",
      "|           Food & Drink| 11|  1|  0|  2|  3|  5|  9| 11|  15|  6|\n",
      "|       Health & Fitness| 21|  1|  3|  4|  2|  7| 12| 25|  81| 24|\n",
      "|              Education| 66|  2|  3|  6| 16| 25| 70|107| 134| 24|\n",
      "|                 Travel| 10|  1|  1|  4|  3|  5|  9| 19|  24|  5|\n",
      "|                Finance| 33|  0|  6|  3|  6|  8| 14| 10|  20|  4|\n",
      "+-----------------------+---+---+---+---+---+---+---+---+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.crosstab('prime_genre', 'user_rating').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What If I want to get the DataFrame which won’t have duplicate rows of given DataFrame?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n",
      "|      prime_genre|user_rating|\n",
      "+-----------------+-----------+\n",
      "|             Book|        5.0|\n",
      "|       Navigation|        4.5|\n",
      "|         Business|        4.0|\n",
      "|        Utilities|        3.5|\n",
      "|             Book|        1.0|\n",
      "|     Food & Drink|        5.0|\n",
      "|           Sports|        2.5|\n",
      "|            Music|        2.5|\n",
      "|             News|        3.0|\n",
      "|    Photo & Video|        4.5|\n",
      "|          Medical|        4.0|\n",
      "|             Book|        2.5|\n",
      "|        Utilities|        4.0|\n",
      "|Social Networking|        3.5|\n",
      "|           Travel|        3.0|\n",
      "|        Utilities|        2.5|\n",
      "|        Lifestyle|        0.0|\n",
      "|    Photo & Video|        0.0|\n",
      "|           Travel|        1.0|\n",
      "|             News|        3.5|\n",
      "+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select('prime_genre','user_rating').dropDuplicates().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if I want to drop the all rows with null value? \n",
    "The dropna operation can be use here. To drop row from the DataFrame it consider three options. \n",
    "* how– ‘any’ or ‘all’. If ‘any’, drop a row if it contains any nulls. If ‘all’, drop a row only if all its values are null.\n",
    "* thresh – int, default None If specified, drop rows that have less than thresh non-null values. This overwrites the how parameter.\n",
    "* subset – optional list of column names to consider. \n",
    "\n",
    "Let’t drop null rows in train with default parameters and count the rows in output DataFrame. Default options are any, None, None for how, thresh, subset respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7197"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train.dropna().count()\n",
    "\n",
    "train = train.withColumnRenamed(\"sup_devices.num\", \"sup_devices_num\")\n",
    "train = train.withColumnRenamed(\"ipadSc_urls.num\", \"ipadSc_urls_num\")\n",
    "train = train.withColumnRenamed(\"lang.num\", \"lang_num\")\n",
    "\n",
    "train.dropna().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if I want to fill the null values in DataFrame with constant number? \n",
    "Use fillna operation here. The fillna will take two parameters to fill the null values. \n",
    "+ value:\n",
    "  + It will take a dictionary to specify which column will replace with which value.\n",
    "  + A value (int , float, string) for all columns.\n",
    "+ subset: Specify some selected columns.\n",
    "\n",
    "Let’s fill ‘-1’ inplace of null values in train DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+--------+-----+----------------+----------------+-----------+---------------+-----+-----------+-----------------+---------------+---------------+--------+-------+\n",
      "|       id|track_name|size_bytes|currency|price|rating_count_tot|rating_count_ver|user_rating|user_rating_ver|  ver|cont_rating|      prime_genre|sup_devices.num|ipadSc_urls.num|lang.num|vpp_lic|\n",
      "+---------+----------+----------+--------+-----+----------------+----------------+-----------+---------------+-----+-----------+-----------------+---------------+---------------+--------+-------+\n",
      "|284882215|  Facebook| 389879808|     USD|  0.0|         2974676|             212|        3.5|            3.5|   95|         4+|Social Networking|             37|              1|      29|      1|\n",
      "|389801252| Instagram| 113954816|     USD|  0.0|         2161558|            1289|        4.5|            4.0|10.23|        12+|    Photo & Video|             37|              0|      29|      1|\n",
      "+---------+----------+----------+--------+-----+----------------+----------------+-----------+---------------+-----+-----------+-----------------+---------------+---------------+--------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.fillna(-1).show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If I want to filter the rows in train which has user_rating more than 3? \n",
    "We can apply the filter operation on user_rating column in train DataFrame to filter out the rows with values more than 3. We need to pass a condition. Let’s apply filter on user_rating column in train DataFrame and print the number of rows which has more user_rating than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5483"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.filter(train.user_rating > 3).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to find the mean rating_count_tot of each prime_genre group in train? \n",
    "The groupby operation can be used here to find the mean of rating_count_tot for each prime_genre group in train. Let’s see how can we get the mean rating_count_tot for the ‘rating_count_tot’ column train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------+\n",
      "|      prime_genre|avg(rating_count_tot)|\n",
      "+-----------------+---------------------+\n",
      "|        Education|   2239.2295805739514|\n",
      "|       Navigation|    11853.95652173913|\n",
      "|    Entertainment|    7533.678504672897|\n",
      "|           Sports|   14026.929824561403|\n",
      "|     Food & Drink|   13938.619047619048|\n",
      "|    Photo & Video|   14352.280802292264|\n",
      "|           Travel|   14129.444444444445|\n",
      "|          Finance|   11047.653846153846|\n",
      "|Social Networking|    45498.89820359281|\n",
      "|             Book|            5125.4375|\n",
      "|         Shopping|    18615.32786885246|\n",
      "|        Reference|          22410.84375|\n",
      "| Health & Fitness|    9913.172222222222|\n",
      "|        Utilities|    6863.822580645161|\n",
      "|     Productivity|   8051.3258426966295|\n",
      "|            Games|   13691.996633868463|\n",
      "|            Music|   28842.021739130436|\n",
      "|        Lifestyle|    6161.763888888889|\n",
      "|         Business|    4788.087719298245|\n",
      "|         Catalogs|               1732.5|\n",
      "+-----------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupby('prime_genre').agg({'rating_count_tot': 'mean'}).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply sum, min, max, count with groupby when we want to get different summary insight each group. Let’s take one more example of groupby to count the number of rows in each Age group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|user_rating|count|\n",
      "+-----------+-----+\n",
      "|        0.0|  929|\n",
      "|        3.5|  702|\n",
      "|        4.5| 2663|\n",
      "|        2.5|  196|\n",
      "|        1.0|   44|\n",
      "|        4.0| 1626|\n",
      "|        3.0|  383|\n",
      "|        2.0|  106|\n",
      "|        1.5|   56|\n",
      "|        5.0|  492|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupby('user_rating').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to create a sample DataFrame from the base DataFrame? \n",
    "We can use sample operation to take sample of a DataFrame. The sample method on DataFrame will return a DataFrame containing the sample of base DataFrame. The sample method will take 3 parameters.\n",
    "\n",
    "+ withReplacement = True or False to select a observation with or without replacement.\n",
    "+ fraction = x, where x = .5 shows that we want to have 50% data in sample DataFrame.\n",
    "+ seed for reproduce the result\n",
    "\n",
    "Let’s create the two DataFrame t1 and t2 from train, both will have 20% sample of train and count the number of rows in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1447, 1391)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = train.sample(False, 0.2, 42)\n",
    "t2 = train.sample(False, 0.2, 43)\n",
    "t1.count(),t2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to apply map operation on DataFrame columns? \n",
    "We can apply a function on each row of DataFrame using map operation. After applying this function, we get the result in the form of RDD. Let’s apply a map operation on User_ID column of train and print the first 5 elements of mapped RDD(x,1) after applying the function (I am applying lambda function). \n",
    "\n",
    "Spark 2.0 (https://stackoverflow.com/questions/39535447/attributeerror-dataframe-object-has-no-attribute-map) \n",
    "You can't map a dataframe, but you can convert the dataframe to an RDD and map that by doing spark_df.rdd.map(). Prior to Spark 2.0, spark_df.map would alias to spark_df.rdd.map(). With Spark 2.0, you must explicitly call .rdd first.\n",
    "\n",
    "Map vs Filter (https://stackoverflow.com/questions/40459695/map-vs-filter-operations)\n",
    "\n",
    "Map, you pass in a function which returns a value for each element in an array. <strong>The return value of this function represents what an element becomes in our new array.</strong>\n",
    "\n",
    "Filter, you pass in a function which returns either true or false for each element. <strong>If the function that you pass returns true for an element, then that element is included in the final array.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Row(id=284882215), 1),\n",
       " (Row(id=389801252), 1),\n",
       " (Row(id=529479190), 1),\n",
       " (Row(id=420009108), 1),\n",
       " (Row(id=284035177), 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.select('id').rdd.map(lambda x:(x,1)).take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to sort the DataFrame based on column(s)? \n",
    "We can use orderBy operation on DataFrame to get sorted output based on some column. The orderBy operation take two arguments.\n",
    "\n",
    "+ List of columns.\n",
    "+ ascending = True or False for getting the results in ascending or descending order(list in case of more than two columns )\n",
    "\n",
    "Let’s sort the train DataFrame based on ‘Purchase’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------+--------+-----+----------------+----------------+-----------+---------------+-----+-----------+-----------+---------------+---------------+--------+-------+\n",
      "|       id|          track_name|size_bytes|currency|price|rating_count_tot|rating_count_ver|user_rating|user_rating_ver|  ver|cont_rating|prime_genre|sup_devices.num|ipadSc_urls.num|lang.num|vpp_lic|\n",
      "+---------+--------------------+----------+--------+-----+----------------+----------------+-----------+---------------+-----+-----------+-----------+---------------+---------------+--------+-------+\n",
      "|924373886|Crossy Road - End...| 165471232|     USD|  0.0|          669079|            1087|        4.5|            4.5|1.5.4|         9+|      Games|             38|              5|      13|      1|\n",
      "|572395608|        Temple Run 2| 158025728|     USD|  0.0|          295211|              91|        4.5|            4.0| 1.37|         9+|      Games|             38|              5|       1|      1|\n",
      "|479516143|Minecraft: Pocket...| 147787776|     USD| 6.99|          522012|            1148|        4.5|            4.5|  1.1|         9+|      Games|             37|              1|      11|      1|\n",
      "|420009108|          Temple Run|  65921024|     USD|  0.0|         1724546|            3842|        4.5|            4.0|1.6.2|         9+|      Games|             40|              5|       1|      1|\n",
      "|596402997|Despicable Me: Mi...| 147123200|     USD|  0.0|          464312|             444|        4.5|            4.5|4.6.0|         9+|      Games|             38|              5|      16|      1|\n",
      "+---------+--------------------+----------+--------+-----+----------------+----------------+-----------+---------------+-----+-----------+-----------+---------------+---------------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.orderBy(train.cont_rating.desc()).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use withColumn operation to add new column (we can also replace) in base DataFrame and return a new DataFrame. The withColumn operation will take 2 parameters.\n",
    "\n",
    "+ Column name which we want add /replace.\n",
    "+ Expression on column.\n",
    "\n",
    "Let’s see how withColumn works. I am calculating new column name ‘Purchase_new’ in train which is calculated by dviding Purchase column by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+\n",
      "|user_rating|user_rating_new|\n",
      "+-----------+---------------+\n",
      "|        3.5|           1.75|\n",
      "|        4.5|           2.25|\n",
      "|        4.5|           2.25|\n",
      "|        4.5|           2.25|\n",
      "|        4.0|            2.0|\n",
      "+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.withColumn('user_rating_new', train.user_rating /2.0).select('user_rating','user_rating_new').show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Apply SQL Queries on DataFrame? \n",
    "We have already discussed in the above section that DataFrame has additional information about datatypes and names of columns associated with it. Unlike RDD, this additional information allows Spark to run SQL queries on DataFrame. To apply SQL queries on DataFrame first we need to register DataFrame as table. Let’s first register train DataFrame as table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE. spark 2.0.0+\n",
    "train.createOrReplaceTempView('train_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|       id|\n",
      "+---------+\n",
      "|284882215|\n",
      "|389801252|\n",
      "|529479190|\n",
      "|420009108|\n",
      "|284035177|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select id from train_table').show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s get maximum user_rating of each prime_genre group in train_table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+\n",
      "|      prime_genre|max(user_rating)|\n",
      "+-----------------+----------------+\n",
      "|        Education|             5.0|\n",
      "|       Navigation|             5.0|\n",
      "|    Entertainment|             5.0|\n",
      "|           Sports|             5.0|\n",
      "|     Food & Drink|             5.0|\n",
      "|    Photo & Video|             5.0|\n",
      "|           Travel|             5.0|\n",
      "|          Finance|             5.0|\n",
      "|Social Networking|             5.0|\n",
      "|             Book|             5.0|\n",
      "|         Shopping|             5.0|\n",
      "|        Reference|             5.0|\n",
      "| Health & Fitness|             5.0|\n",
      "|        Utilities|             5.0|\n",
      "|     Productivity|             5.0|\n",
      "|            Games|             5.0|\n",
      "|            Music|             5.0|\n",
      "|        Lifestyle|             5.0|\n",
      "|         Business|             5.0|\n",
      "|         Catalogs|             5.0|\n",
      "+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select prime_genre, max(user_rating) from train_table group by prime_genre').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did all data processing steps work well with your new dataset? If no, which instructions did not work and why you think it happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, there was one step which resulted in an error:  \n",
    "\n",
    "#### Instruction:\n",
    "train.dropna().count()\n",
    "\n",
    "#### Error:\n",
    "AnalysisException: 'Cannot resolve column name \"sup_devices.num\" among (id, track_name, size_bytes, currency, price, rating_count_tot, rating_count_ver, user_rating, user_rating_ver, ver, cont_rating, prime_genre, sup_devices.num, ipadSc_urls.num, lang.num, vpp_lic);'\n",
    "\n",
    "#### Why it happened?\n",
    "The error occured due to the column name \"sup_devices.num\" containing a special character (the dot \".\") which can cause confusion in some cases. I tried to handle the error by renaming the column names and the issue was resolved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building three questions to investigate further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is the distribution of the number of supported devices (sup_devices.num) across different genres?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|      prime_genre|   avg_user_rating|\n",
      "+-----------------+------------------+\n",
      "|     Productivity|  4.00561797752809|\n",
      "|            Music|3.9782608695652173|\n",
      "|    Photo & Video|3.8008595988538683|\n",
      "|         Business| 3.745614035087719|\n",
      "| Health & Fitness|               3.7|\n",
      "|            Games|3.6850077679958573|\n",
      "|          Weather|3.5972222222222223|\n",
      "|         Shopping| 3.540983606557377|\n",
      "|        Reference|          3.453125|\n",
      "|           Travel| 3.376543209876543|\n",
      "|        Education| 3.376379690949227|\n",
      "|          Medical| 3.369565217391304|\n",
      "|        Utilities| 3.278225806451613|\n",
      "|    Entertainment|3.2467289719626167|\n",
      "|     Food & Drink|3.1825396825396823|\n",
      "|Social Networking|2.9850299401197606|\n",
      "|           Sports| 2.982456140350877|\n",
      "|             News|              2.98|\n",
      "|        Lifestyle|2.8055555555555554|\n",
      "|       Navigation|2.6847826086956523|\n",
      "+-----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('SELECT prime_genre, AVG(user_rating) AS avg_user_rating \\\n",
    "                FROM train_table \\\n",
    "                GROUP BY prime_genre \\\n",
    "                ORDER BY avg_user_rating DESC').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. How does the average price of apps vary across different content ratings (cont_rating)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|cont_rating|         avg_price|\n",
      "+-----------+------------------+\n",
      "|        12+|1.5666666666666695|\n",
      "|        17+|0.9811093247588443|\n",
      "|         4+|1.7772005413940217|\n",
      "|         9+|2.1535055724417433|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('SELECT cont_rating, AVG(price) AS avg_price \\\n",
    "                FROM train_table \\\n",
    "                GROUP BY cont_rating \\\n",
    "                ORDER BY cont_rating').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the relationship between app size and user ratings for each prime_genre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+------------------+\n",
      "|      prime_genre|        avg_app_size|   avg_user_rating|\n",
      "+-----------------+--------------------+------------------+\n",
      "|             Book| 1.788206262857143E8|2.4776785714285716|\n",
      "|         Business|  6.41684929122807E7| 3.745614035087719|\n",
      "|         Catalogs|         5.0181632E7|               2.1|\n",
      "|        Education|1.8042422133995584E8| 3.376379690949227|\n",
      "|    Entertainment|1.0147872102056074E8|3.2467289719626167|\n",
      "|          Finance| 7.823585928846154E7|2.4326923076923075|\n",
      "|     Food & Drink| 7.759499784126984E7|3.1825396825396823|\n",
      "|            Games|2.8365830049792856E8|3.6850077679958573|\n",
      "| Health & Fitness| 9.010664106666666E7|               3.7|\n",
      "|        Lifestyle| 6.230646750694445E7|2.8055555555555554|\n",
      "|          Medical| 3.763890086956522E8| 3.369565217391304|\n",
      "|            Music|1.0963563757971014E8|3.9782608695652173|\n",
      "|       Navigation| 1.033544852826087E8|2.6847826086956523|\n",
      "|             News|6.2470853266666666E7|              2.98|\n",
      "|    Photo & Video| 6.852190477936962E7|3.8008595988538683|\n",
      "|     Productivity| 7.844063784269662E7|  4.00561797752809|\n",
      "|        Reference|    1.551626831875E8|          3.453125|\n",
      "|         Shopping| 9.334683278688525E7| 3.540983606557377|\n",
      "|Social Networking| 7.937243956287426E7|2.9850299401197606|\n",
      "|           Sports| 7.887357305263157E7| 2.982456140350877|\n",
      "+-----------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('SELECT prime_genre, AVG(size_bytes) AS avg_app_size, AVG(user_rating) AS avg_user_rating \\\n",
    "                FROM train_table \\\n",
    "                GROUP BY prime_genre \\\n",
    "                ORDER BY prime_genre').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOURCE  of This Notebook\n",
    "https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
