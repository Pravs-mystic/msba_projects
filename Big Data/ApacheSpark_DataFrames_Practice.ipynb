{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOURCE  of This Notebook\n",
    "https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/ \n",
    "#### Data file download - sign in required with registration\n",
    "https://www.kaggle.com/sdolezel/black-friday\n",
    "Once logged in, 'Download' link will be available around top-right corner\n",
    "#### In the downloaded zip file, you will upload 'train' data file to Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns\n",
    "User_ID    User ID  \n",
    "Product_ID      Product ID  \n",
    "Gender     Sex of User  \n",
    "Age   Age in bins  \n",
    "Occupation      Occupation (Masked)  \n",
    "City_Category   Category of the City (A,B,C)  \n",
    "Stay_In_Current_City_Years     Number of years stay in current city  \n",
    "Marital_Status Marital Status  \n",
    "Product_Category_1  \n",
    "Product_Category_2  \n",
    "Product_Category_3  \n",
    "Purchase   Purchase Amount "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Apache Spark Context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the DataFrame from CSV file\n",
    "#### This is where you import your own csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sqlContext.read.csv(\"train.csv\", header = True, inferSchema = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to see datatype of columns?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: integer (nullable = true)\n",
      " |-- Product_Category_3: integer (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Show first n observation? \n",
    "We can use head operation to see first n observation (say, 5 observation). Head operation in PySpark is similar to head operation in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(User_ID=1000001, Product_ID='P00069042', Gender='F', Age='0-17', Occupation=10, City_Category='A', Stay_In_Current_City_Years='2', Marital_Status=0, Product_Category_1=3, Product_Category_2=None, Product_Category_3=None, Purchase=8370),\n",
       " Row(User_ID=1000001, Product_ID='P00248942', Gender='F', Age='0-17', Occupation=10, City_Category='A', Stay_In_Current_City_Years='2', Marital_Status=0, Product_Category_1=1, Product_Category_2=6, Product_Category_3=14, Purchase=15200),\n",
       " Row(User_ID=1000001, Product_ID='P00087842', Gender='F', Age='0-17', Occupation=10, City_Category='A', Stay_In_Current_City_Years='2', Marital_Status=0, Product_Category_1=12, Product_Category_2=None, Product_Category_3=None, Purchase=1422),\n",
       " Row(User_ID=1000001, Product_ID='P00085442', Gender='F', Age='0-17', Occupation=10, City_Category='A', Stay_In_Current_City_Years='2', Marital_Status=0, Product_Category_1=12, Product_Category_2=14, Product_Category_3=None, Purchase=1057),\n",
       " Row(User_ID=1000002, Product_ID='P00285442', Gender='M', Age='55+', Occupation=16, City_Category='C', Stay_In_Current_City_Years='4+', Marital_Status=0, Product_Category_1=8, Product_Category_2=None, Product_Category_3=None, Purchase=7969)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above results are comprised of row like format. To see the result in more interactive manner (rows under the columns), we can use the show operation. Let’s apply show operation on train and take first 2 rows of it. We can pass the argument truncate = True to truncate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(2,truncate= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Count the number of rows in DataFrame?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550068"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many columns do we have in train and test files along with their names?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,\n",
       " ['User_ID',\n",
       "  'Product_ID',\n",
       "  'Gender',\n",
       "  'Age',\n",
       "  'Occupation',\n",
       "  'City_Category',\n",
       "  'Stay_In_Current_City_Years',\n",
       "  'Marital_Status',\n",
       "  'Product_Category_1',\n",
       "  'Product_Category_2',\n",
       "  'Product_Category_3',\n",
       "  'Purchase'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.columns), train.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to get the summary statistics (mean, standard deviance, min ,max, count) of numerical columns in a DataFrame? \n",
    "describe operation is use to calculate the summary statistics of numerical column(s) in DataFrame. If we don’t specify the name of columns it will calculate summary statistics for all numerical columns present in DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|           User_ID|Product_ID|Gender|   Age|       Occupation|City_Category|Stay_In_Current_City_Years|     Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|         Purchase|\n",
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            550068|    550068|550068|550068|           550068|       550068|                    550068|             550068|            550068|            376430|            166821|           550068|\n",
      "|   mean|1003028.8424013031|      null|  null|  null|8.076706879876669|         null|         1.468494139793958|0.40965298835780306| 5.404270017525106| 9.842329251122386|12.668243206790512|9263.968712959126|\n",
      "| stddev|1727.5915855313747|      null|  null|  null|6.522660487341741|         null|        0.9890866807573103| 0.4917701263173315| 3.936211369201365| 5.086589648693497| 4.125337631575274|5023.065393820575|\n",
      "|    min|           1000001| P00000142|     F|  0-17|                0|            A|                         0|                  0|                 1|                 2|                 3|               12|\n",
      "|    max|           1006040|  P0099942|     M|   55+|               20|            C|                        4+|                  1|                20|                18|                18|            23961|\n",
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|summary|Product_ID|\n",
      "+-------+----------+\n",
      "|  count|    550068|\n",
      "|   mean|      null|\n",
      "| stddev|      null|\n",
      "|    min| P00000142|\n",
      "|    max|  P0099942|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.describe('Product_ID').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to select column(s) from the DataFrame?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|User_ID| Age|\n",
      "+-------+----+\n",
      "|1000001|0-17|\n",
      "|1000001|0-17|\n",
      "|1000001|0-17|\n",
      "|1000001|0-17|\n",
      "|1000002| 55+|\n",
      "+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select('User_ID','Age').show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if I want to calculate pair wise frequency of categorical columns? \n",
    "We can use crosstab operation on DataFrame to calculate the pair wise frequency of columns. Let’s apply crosstab operation on ‘Age’ and ‘Gender’ columns of train DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+\n",
      "|Age_Gender|    F|     M|\n",
      "+----------+-----+------+\n",
      "|      0-17| 5083| 10019|\n",
      "|     46-50|13199| 32502|\n",
      "|     18-25|24628| 75032|\n",
      "|     36-45|27170| 82843|\n",
      "|       55+| 5083| 16421|\n",
      "|     51-55| 9894| 28607|\n",
      "|     26-35|50752|168835|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.crosstab('Age', 'Gender').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What If I want to get the DataFrame which won’t have duplicate rows of given DataFrame?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|  Age|Gender|\n",
      "+-----+------+\n",
      "|51-55|     F|\n",
      "|18-25|     M|\n",
      "| 0-17|     F|\n",
      "|46-50|     M|\n",
      "|18-25|     F|\n",
      "|  55+|     M|\n",
      "|  55+|     F|\n",
      "|36-45|     M|\n",
      "|26-35|     F|\n",
      "| 0-17|     M|\n",
      "|36-45|     F|\n",
      "|51-55|     M|\n",
      "|26-35|     M|\n",
      "|46-50|     F|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select('Age','Gender').dropDuplicates().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if I want to drop the all rows with null value? \n",
    "The dropna operation can be use here. To drop row from the DataFrame it consider three options. \n",
    "* how– ‘any’ or ‘all’. If ‘any’, drop a row if it contains any nulls. If ‘all’, drop a row only if all its values are null.\n",
    "* thresh – int, default None If specified, drop rows that have less than thresh non-null values. This overwrites the how parameter.\n",
    "* subset – optional list of column names to consider. \n",
    "\n",
    "Let’t drop null rows in train with default parameters and count the rows in output DataFrame. Default options are any, None, None for how, thresh, subset respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166821"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dropna().count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if I want to fill the null values in DataFrame with constant number? \n",
    "Use fillna operation here. The fillna will take two parameters to fill the null values. \n",
    "+ value:\n",
    "  + It will take a dictionary to specify which column will replace with which value.\n",
    "  + A value (int , float, string) for all columns.\n",
    "+ subset: Specify some selected columns.\n",
    "\n",
    "Let’s fill ‘-1’ inplace of null values in train DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|                -1|                -1|    8370|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.fillna(-1).show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If I want to filter the rows in train which has Purchase more than 15000? \n",
    "We can apply the filter operation on Purchase column in train DataFrame to filter out the rows with values more than 15000. We need to pass a condition. Let’s apply filter on Purchase column in train DataFrame and print the number of rows which has more purchase than 15000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110523"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.filter(train.Purchase > 15000).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to find the mean of each age group in train? \n",
    "The groupby operation can be used here to find the mean of Purchase for each age group in train. Let’s see how can we get the mean purchase for the ‘Age’ column train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|  Age|    avg(Purchase)|\n",
      "+-----+-----------------+\n",
      "|18-25|9169.663606261289|\n",
      "|26-35|9252.690632869888|\n",
      "| 0-17|8933.464640444974|\n",
      "|46-50|9208.625697468327|\n",
      "|51-55|9534.808030960236|\n",
      "|36-45|9331.350694917874|\n",
      "|  55+|9336.280459449405|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupby('Age').agg({'Purchase': 'mean'}).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply sum, min, max, count with groupby when we want to get different summary insight each group. Let’s take one more example of groupby to count the number of rows in each Age group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|  Age| count|\n",
      "+-----+------+\n",
      "|18-25| 99660|\n",
      "|26-35|219587|\n",
      "| 0-17| 15102|\n",
      "|46-50| 45701|\n",
      "|51-55| 38501|\n",
      "|36-45|110013|\n",
      "|  55+| 21504|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupby('Age').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to create a sample DataFrame from the base DataFrame? \n",
    "We can use sample operation to take sample of a DataFrame. The sample method on DataFrame will return a DataFrame containing the sample of base DataFrame. The sample method will take 3 parameters.\n",
    "\n",
    "+ withReplacement = True or False to select a observation with or without replacement.\n",
    "+ fraction = x, where x = .5 shows that we want to have 50% data in sample DataFrame.\n",
    "+ seed for reproduce the result\n",
    "\n",
    "Let’s create the two DataFrame t1 and t2 from train, both will have 20% sample of train and count the number of rows in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109839, 109861)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = train.sample(False, 0.2, 42)\n",
    "t2 = train.sample(False, 0.2, 43)\n",
    "t1.count(),t2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to apply map operation on DataFrame columns? \n",
    "We can apply a function on each row of DataFrame using map operation. After applying this function, we get the result in the form of RDD. Let’s apply a map operation on User_ID column of train and print the first 5 elements of mapped RDD(x,1) after applying the function (I am applying lambda function). \n",
    "\n",
    "Spark 2.0 (https://stackoverflow.com/questions/39535447/attributeerror-dataframe-object-has-no-attribute-map) \n",
    "You can't map a dataframe, but you can convert the dataframe to an RDD and map that by doing spark_df.rdd.map(). Prior to Spark 2.0, spark_df.map would alias to spark_df.rdd.map(). With Spark 2.0, you must explicitly call .rdd first.\n",
    "\n",
    "Map vs Filter (https://stackoverflow.com/questions/40459695/map-vs-filter-operations)\n",
    "\n",
    "Map, you pass in a function which returns a value for each element in an array. <strong>The return value of this function represents what an element becomes in our new array.</strong>\n",
    "\n",
    "Filter, you pass in a function which returns either true or false for each element. <strong>If the function that you pass returns true for an element, then that element is included in the final array.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Row(User_ID=1000001), 1),\n",
       " (Row(User_ID=1000001), 1),\n",
       " (Row(User_ID=1000001), 1),\n",
       " (Row(User_ID=1000001), 1),\n",
       " (Row(User_ID=1000002), 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.select('User_ID').rdd.map(lambda x:(x,1)).take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to sort the DataFrame based on column(s)? \n",
    "We can use orderBy operation on DataFrame to get sorted output based on some column. The orderBy operation take two arguments.\n",
    "\n",
    "+ List of columns.\n",
    "+ ascending = True or False for getting the results in ascending or descending order(list in case of more than two columns )\n",
    "\n",
    "Let’s sort the train DataFrame based on ‘Purchase’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1003160| P00052842|     M|26-35|        17|            C|                         3|             0|                10|                15|              null|   23961|\n",
      "|1002272| P00052842|     M|26-35|         0|            C|                         1|             0|                10|                15|              null|   23961|\n",
      "|1001474| P00052842|     M|26-35|         4|            A|                         2|             1|                10|                15|              null|   23961|\n",
      "|1003045| P00052842|     M|46-50|         1|            B|                         2|             1|                10|                15|              null|   23960|\n",
      "|1005596| P00117642|     M|36-45|        12|            B|                         1|             0|                10|                16|              null|   23960|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.orderBy(train.Purchase.desc()).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use withColumn operation to add new column (we can also replace) in base DataFrame and return a new DataFrame. The withColumn operation will take 2 parameters.\n",
    "\n",
    "+ Column name which we want add /replace.\n",
    "+ Expression on column.\n",
    "\n",
    "Let’s see how withColumn works. I am calculating new column name ‘Purchase_new’ in train which is calculated by dviding Purchase column by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|Purchase|Purchase_new|\n",
      "+--------+------------+\n",
      "|    8370|      4185.0|\n",
      "|   15200|      7600.0|\n",
      "|    1422|       711.0|\n",
      "|    1057|       528.5|\n",
      "|    7969|      3984.5|\n",
      "+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.withColumn('Purchase_new', train.Purchase /2.0).select('Purchase','Purchase_new').show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Apply SQL Queries on DataFrame? \n",
    "We have already discussed in the above section that DataFrame has additional information about datatypes and names of columns associated with it. Unlike RDD, this additional information allows Spark to run SQL queries on DataFrame. To apply SQL queries on DataFrame first we need to register DataFrame as table. Let’s first register train DataFrame as table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE. spark 2.0.0+\n",
    "train.createOrReplaceTempView('train_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Product_ID|\n",
      "+----------+\n",
      "| P00069042|\n",
      "| P00248942|\n",
      "| P00087842|\n",
      "| P00085442|\n",
      "| P00285442|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select Product_ID from train_table').show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s get maximum purchase of each Age group in train_table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|  Age|max(Purchase)|\n",
      "+-----+-------------+\n",
      "|18-25|        23958|\n",
      "|26-35|        23961|\n",
      "| 0-17|        23955|\n",
      "|46-50|        23960|\n",
      "|51-55|        23960|\n",
      "|36-45|        23960|\n",
      "|  55+|        23960|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select Age, max(Purchase) from train_table group by Age').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOURCE  of This Notebook\n",
    "https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
